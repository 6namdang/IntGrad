{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to clean and preprocess data\n",
    "def clean_and_save_data(csv_path, output_path):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    # Check for valid pixel data\n",
    "    data['pixel_length'] = data['pixels'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "    valid_data = data[data['pixel_length'] == 2304]  # Keep rows with 48x48 = 2304 pixels\n",
    "    valid_data = valid_data.drop(columns=['pixel_length'])\n",
    "    valid_data.to_csv(output_path, index=False)\n",
    "    print(\"Cleaned dataset saved at:\", output_path)\n",
    "\n",
    "# Function to load and preprocess data from CSV\n",
    "def load_data_from_csv(csv_path, sequence_length=10, img_height=48, img_width=48, num_classes=7):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - sequence_length, sequence_length):\n",
    "        sequence = []\n",
    "        valid_sequence = True\n",
    "        for j in range(sequence_length):\n",
    "            pixels = data.iloc[i + j]['pixels']\n",
    "            if pd.isnull(pixels) or len(pixels.split()) != 2304:\n",
    "                valid_sequence = False\n",
    "                break\n",
    "            pixels = np.array([int(p) for p in pixels.split()]).reshape(img_height, img_width, 1)\n",
    "            sequence.append(pixels)\n",
    "        if valid_sequence:\n",
    "            X.append(sequence)\n",
    "            y.append(data.iloc[i]['emotion'])\n",
    "    X = np.array(X) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    y = tf.keras.utils.to_categorical(np.array(y), num_classes=num_classes)  # One-hot encode labels\n",
    "    return X, y\n",
    "\n",
    "# Paths\n",
    "input_csv_path = 'Dataset/fer2013.csv'\n",
    "cleaned_csv_path = 'Dataset/fer2013_cleaned.csv'\n",
    "\n",
    "# Step 1: Clean and save the data\n",
    "clean_and_save_data(input_csv_path, cleaned_csv_path)\n",
    "\n",
    "# Step 2: Load cleaned data\n",
    "X, y = load_data_from_csv(cleaned_csv_path)\n",
    "\n",
    "# Step 3: Split the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 4: Define the CNN-LSTM model\n",
    "num_classes = 7  # FER2013 has 7 classes of emotions\n",
    "model = Sequential()\n",
    "\n",
    "# TimeDistributed CNN for feature extraction from each frame\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=(None, 48, 48, 1)))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# LSTM for temporal feature learning\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected layer for classification\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Step 5: Train the model\n",
    "if len(X_train) > 0 and len(y_train) > 0:\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "    # Step 6: Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Step 7: Evaluate the model on the test set\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data to train the model.\")\n",
    "\n",
    "# Step 8: Visualize sample images (Optional)\n",
    "for i in range(5):\n",
    "    img = X[i][0].reshape(48, 48)  # First frame of the sequence\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"Emotion: {np.argmax(y[i])}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
